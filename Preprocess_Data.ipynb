{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Sets up directory structure.\n",
    "!mkdir -p DATA/{PREPROCESSED/TEST,PREPROCESSED/TRAIN,ORIGINAL/.tmp,GENERATED}\n",
    "\n",
    "# Save\n",
    "import arff\n",
    "def save_arff(df,filename):\n",
    "    filename=filename.replace(\".csv\",\".arff\")\n",
    "    arff.dump(filename, df.values, relation='Relation', names=df.columns)\n",
    "    if len(df.Class.unique()) == 1: \n",
    "        print(\"=1 class:\",df.Class.unique())\n",
    "    else:\n",
    "        print(\">1 class:\",df.Class.unique())\n",
    "        \n",
    "    _max = 10 if \"MNIST\" in filename else 2\n",
    "    classes = \"{\" + \",\".join([str(x) for x in range(_max)]) + \"}\"\n",
    "    !sed -i \"s/Class string/Class {classes}/g\" {filename}\n",
    "    \n",
    "    \n",
    "def save_data(data=None, save_file=None, attribute_names=None, num_classes=1):\n",
    "    #invalid attribute names for weka\n",
    "    if data.shape[1] ==36:attribute_names=[\"attr_\"+str(i+1) for i in range (len(attribute_names))]\n",
    "    attributes = data[:, :-num_classes]\n",
    "    classes    = data[:, -num_classes:]\n",
    "\n",
    "    if num_classes > 1:\n",
    "        # MNIST class is the index of largest row.\n",
    "        _class = np.argmax(classes,axis=1)\n",
    "    else:\n",
    "        # Other class are defined by the conditions x>=.5 -> 1 and x<.5 -> 0\n",
    "        _class = np.around(classes)\n",
    "    print(save_file,\"mean:\",np.mean(_class),\"| std:\",np.std(_class),end=\" | \")\n",
    "    # Last column must be Class as int.\n",
    "    df=pd.DataFrame(attributes,columns=attribute_names)\n",
    "#     print(_class)\n",
    "    df[\"Class\"]=_class.astype(int).astype(str)\n",
    "    df.to_csv(save_file,index=False)\n",
    "    save_arff(df, save_file)\n",
    "    \n",
    "    # If saving the MNIST_data also save the expanded class for training on the wgan.\n",
    "    if num_classes > 1:\n",
    "        df=pd.DataFrame(attributes,columns=attribute_names)\n",
    "        for i in range(num_classes):\n",
    "            df[\"Class_{}\".format(i)]=classes[:,i]\n",
    "        df.to_csv(save_file.replace(\".csv\",\"_for_DPWGAN.csv\"),index=False)\n",
    "\n",
    "\n",
    "# Splits data into 70% training set and 30% testing set.\n",
    "def split_np_array(array): \n",
    "    return array[:int(.7*array.shape[0])], array[int(.7*array.shape[0]):]\n",
    "\n",
    "\n",
    "# Stadardize each column independently to 0<x<1.\n",
    "def standardize_columns(array):\n",
    "    def _standardize_columns(x):\n",
    "        if np.all(x==0.0): return x\n",
    "        _max=np.amax(x)\n",
    "        _min=np.amin(x)\n",
    "        x = (x-_min)/(_max-_min)\n",
    "        return x\n",
    "    for i in range(array.shape[1]-1):\n",
    "        array[:,i:i+1] = _standardize_columns(array[:,i:i+1])\n",
    "    return array\n",
    "\n",
    "    \n",
    "def preprocess_MNIST_data():\n",
    "    \"\"\"\n",
    "    Original Data downloaded through keras\n",
    "    \"\"\"\n",
    "    from keras.datasets import mnist\n",
    "    global train_mnist\n",
    "    # load (downloaded if needed) the MNIST dataset\n",
    "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "    Y = np.expand_dims(np.array(list(y_train) + list(y_test),dtype=int),-1)\n",
    "    Y_expanded = np.zeros((len(Y),10),dtype=float)\n",
    "    for i,x in enumerate(list(Y)): \n",
    "        Y_expanded[i][x]=1.0\n",
    "    X = np.concatenate([X_train.flatten(),X_test.flatten()])\n",
    "    X = np.reshape(X,(-1,28*28)).astype(float)/255\n",
    "    data_mnist = np.concatenate([X,Y_expanded], axis=1)\n",
    "    train_mnist,test_mnist = split_np_array(data_mnist)\n",
    "    \n",
    "    #crate lables for columns\n",
    "    MNIST_attribute_names = []\n",
    "    for i in range(28):\n",
    "        for j in range(28):\n",
    "            cell_name = \"Cell_{}_{}\".format(i,j)\n",
    "            MNIST_attribute_names += [cell_name]\n",
    "    \n",
    "    # save data\n",
    "    save_data(data=train_mnist, save_file=\"DATA/PREPROCESSED/TRAIN/MNIST_TRAIN.csv\", \n",
    "              attribute_names=MNIST_attribute_names, num_classes=10)\n",
    "    save_data(data=test_mnist,  save_file=\"DATA/PREPROCESSED/TEST/MNIST_TEST.csv\",  \n",
    "              attribute_names=MNIST_attribute_names, num_classes=10)\n",
    "    print(\"Preprocessed MNIST data\")\n",
    "    \n",
    "\n",
    "def preprocess_KCCR_data():\n",
    "    \"\"\"\n",
    "    The file with original data is DATA/ORIGINAL/Kaggle_Cervical_Cancer_Risk.csv\n",
    "    Note: First, used excel to replace some missing values in the \"STDs: Time since first diagnosis\" and \"STDs: Time since last diagnosis\"\n",
    "    in pandas style psudo-code: \n",
    "    for row in KCCR:\n",
    "        set KCCR_df[\"STDs: Time since first diagnosis\"] = KCCR_df[\"Stds\"] if KCCR_df[\"Stds\"] == 0 \n",
    "        set KCCR_df[\"STDs: Time since last diagnosis\"] = KCCR_df[\"Stds\"] if KCCR_df[\"Stds\"] == 0\n",
    "    The file with these changes is DATA/ORIGINAL/.tmp/Kaggle_Cervical_Cancer_Risk.csv\n",
    "    \"\"\"   \n",
    "    global train_kccr\n",
    "    # Replaces \"?\" with \"\" so that it loads into pandas with np.nan values.\n",
    "    !cat DATA/ORIGINAL/.tmp/Kaggle_Cervical_Cancer_Risk.csv | sed s/\\?//g >DATA/ORIGINAL/.tmp/Kaggle_Cervical_Cancer_Risk2.csv\n",
    "    KCCR_df  = pd.read_csv(\"DATA/ORIGINAL/.tmp/Kaggle_Cervical_Cancer_Risk2.csv\").astype(float)\n",
    "    for x in list(KCCR_df): # For each column, replace nan values with the column average\n",
    "        ave = KCCR_df[x].dropna().mean()\n",
    "        KCCR_df[x]=KCCR_df[x].fillna(ave)\n",
    "    KCCR_attribute_names  = list(KCCR_df)[:-1]\n",
    "    \n",
    "    \n",
    "    data_kccr = KCCR_df.values.astype(float)\n",
    "    data_kccr = standardize_columns(data_kccr)\n",
    "    train_kccr, test_kccr  = split_np_array(data_kccr)\n",
    "    print(len([x for x in data_kccr if x[-1]==1]),len([x for x in data_kccr]))\n",
    "    # duplicate positive instances untill they compose 1/5 of data\n",
    "    pos=[x for x in train_kccr if x[-1]==1]\n",
    "    neg=[x for x in train_kccr if x[-1]==0]\n",
    "    pos=pos*4\n",
    "    train_kccr=np.asarray(neg+pos)\n",
    "    np.random.shuffle(train_kccr)\n",
    "    \n",
    "    # save data\n",
    "    save_data(data=train_kccr, save_file=\"DATA/PREPROCESSED/TRAIN/KCCR_TRAIN.csv\", \n",
    "              attribute_names=KCCR_attribute_names, num_classes=1)\n",
    "    save_data(data=test_kccr,  save_file=\"DATA/PREPROCESSED/TEST/KCCR_TEST.csv\",  \n",
    "              attribute_names=KCCR_attribute_names, num_classes=1)\n",
    "    print(\"Preprocessed KCCR data\")\n",
    "    \n",
    "\n",
    "def preprocess_KCCFD_data():\n",
    "    \"\"\"\n",
    "    The file with original data is DATA/ORIGINAL/Kaggle_Credit_Card_Fraud_Detection.csv\n",
    "    \"\"\"\n",
    "    global train_kccfd\n",
    "    KCCFD_path = \"DATA/ORIGINAL/Kaggle_Credit_Card_Fraud_Detection.csv\"\n",
    "    KCCFD_df = pd.read_csv(KCCFD_path)\n",
    "    #cutoff time variable\n",
    "    KCCFD_attribute_names = list(KCCFD_df)[1:-1]\n",
    "    data_kccfd = KCCFD_df.values[:,1:]\n",
    "    print(\"The number of positive values in the KCCFD dataset is:\",\n",
    "          len([x for x in data_kccfd if x[-1]==1]))\n",
    "    \n",
    "    data_kccfd = standardize_columns(data_kccfd)\n",
    "    train_kccfd,test_kccfd = split_np_array(data_kccfd)\n",
    "    print(len([x for x in data_kccfd if x[-1]==1]),len([x for x in data_kccfd]))\n",
    "    \n",
    "    # duplicate positive instances untill they compose 1/5 of data\n",
    "    pos=[x for x in train_kccfd if x[-1]==1]\n",
    "    neg=[x for x in train_kccfd if x[-1]==0]\n",
    "    print(\"int(len(neg)/(4*len(pos)))\",int(len(neg)/(4*len(pos))))\n",
    "    pos=pos*int(len(neg)/(4*len(pos)))\n",
    "    train_kccfd=np.asarray(neg+pos)\n",
    "    np.random.shuffle(train_kccfd)\n",
    "    # save data\n",
    "    save_data(data=train_kccfd, save_file=\"DATA/PREPROCESSED/TRAIN/KCCFD_TRAIN.csv\", \n",
    "              attribute_names=KCCFD_attribute_names, num_classes=1)\n",
    "    save_data(data=test_kccfd,  save_file=\"DATA/PREPROCESSED/TEST/KCCFD_TEST.csv\",  \n",
    "              attribute_names=KCCFD_attribute_names, num_classes=1)\n",
    "    print(\"Preprocessed KCCFD data\")\n",
    "    \n",
    "    \n",
    "def preprocess_data():\n",
    "    preprocess_KCCR_data()\n",
    "    preprocess_KCCFD_data()\n",
    "    preprocess_MNIST_data()\n",
    "preprocess_data()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
