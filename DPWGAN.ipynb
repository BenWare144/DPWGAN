{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# other stuff\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 50)\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# \"\"\"Minimal implementation of Wasserstein GAN for MNIST.\"\"\"\n",
    "from tensorflow.contrib import layers\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import itertools\n",
    "from utils import *\n",
    "\n",
    "# Save\n",
    "import arff\n",
    "def save_arff(df,filename):\n",
    "    filename=filename.replace(\".csv\",\".arff\")\n",
    "    arff.dump(filename, df.values, relation='Relation', names=df.columns)\n",
    "    if len(df.Class.unique()) == 1: \n",
    "        print(\"=1 class:\",df.Class.unique())\n",
    "    else:\n",
    "        print(\">1 class:\",df.Class.unique())\n",
    "        \n",
    "    _max = 10 if \"MNIST\" in filename else 2\n",
    "    classes = \"{\" + \",\".join([str(x) for x in range(_max)]) + \"}\"\n",
    "    !sed -i \"s/Class string/Class {classes}/g\" {filename}\n",
    "    \n",
    "def save_data(data=None, save_file=None, attribute_names=None, num_classes=1):\n",
    "    #invalid attribute names for weka\n",
    "    if data.shape[1] ==36:attribute_names=[\"attr_\"+str(i+1) for i in range (len(attribute_names))]\n",
    "    attributes = data[:, :-num_classes]\n",
    "    classes    = data[:, -num_classes:]\n",
    "\n",
    "    if num_classes > 1:\n",
    "        # MNIST class is the index of largest row.\n",
    "        _class = np.argmax(classes,axis=1)\n",
    "    else:\n",
    "        # Other class are defined by the conditions x>=.5 -> 1 and x<.5 -> 0\n",
    "        _class = np.around(classes)\n",
    "    print(save_file,\"mean:\",np.mean(_class),\"| std:\",np.std(_class),end=\" | \")\n",
    "    # Last column must be Class as int.\n",
    "    df=pd.DataFrame(attributes,columns=attribute_names)\n",
    "#     print(_class)\n",
    "    df[\"Class\"]=_class.astype(int).astype(str)\n",
    "    df.to_csv(save_file,index=False)\n",
    "    save_arff(df, save_file)\n",
    "    \n",
    "    # If saving the MNIST_data also save the expanded class for training on the wgan.\n",
    "    if num_classes > 1:\n",
    "        df=pd.DataFrame(attributes,columns=attribute_names)\n",
    "        for i in range(num_classes):\n",
    "            df[\"Class_{}\".format(i)]=classes[:,i]\n",
    "        df.to_csv(save_file.replace(\".csv\",\"_for_DPWGAN.csv\"),index=False)\n",
    "\n",
    "\n",
    "class Data_Loader:\n",
    "    def __init__(self,data):\n",
    "        self.data = data\n",
    "        self.data_len = data.shape[0]\n",
    "        self.i = -1\n",
    "    def next_batch(self, batch_size):\n",
    "        max_i=int(len(self.data)/batch_size)\n",
    "        self.i += 1\n",
    "        if self.i == max_i: self.i = 0\n",
    "        return self.data[batch_size*self.i: (self.i+1)*batch_size]\n",
    "\n",
    "# Use this if you don't want to preprocess data again.\n",
    "def Load_data():\n",
    "    # Originally data loaded into global varibles during preproccessing.\n",
    "    global attribute_names, all_data\n",
    "    train_df_kccfd=pd.read_csv(\"DATA/PREPROCESSED/TRAIN/KCCFD_TRAIN.csv\")\n",
    "    train_df_kccr=pd.read_csv(\"DATA/PREPROCESSED/TRAIN/KCCR_TRAIN.csv\")\n",
    "    train_df_mnist=pd.read_csv(\"DATA/PREPROCESSED/TRAIN/MNIST_TRAIN_for_DPWGAN.csv\")\n",
    "    attribute_names={}\n",
    "    attribute_names[\"KCCR\"]=list(train_df_kccr)[:-1]\n",
    "    attribute_names[\"KCCFD\"]=list(train_df_kccfd)[:-1]\n",
    "    attribute_names[\"MNIST\"]=list(train_df_mnist)[:-10]\n",
    "    train_mnist=train_df_mnist.values\n",
    "    train_kccfd=train_df_kccfd.values\n",
    "    train_kccr=train_df_kccr.values\n",
    "    all_data={\"MNIST\": Data_Loader(train_mnist),\n",
    "          \"KCCFD\": Data_Loader(train_kccfd),\n",
    "          \"KCCR\":  Data_Loader(train_kccr)}  \n",
    "Load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     206
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#network archetecture\n",
    "def get_nets(N):\n",
    "    \n",
    "    def leaky_relu(x): return tf.maximum(x, 0.2 * x)\n",
    "    \n",
    "    if N==\"MNIST\":\n",
    "        # Weights based on the implementation here:\n",
    "        # https://github.com/Zackory/Keras-MNIST-GAN/blob/master/mnist_gan.py\n",
    "        def generator(z):\n",
    "            with tf.variable_scope('generator'):\n",
    "                z = layers.fully_connected(z, num_outputs=256, activation_fn=leaky_relu)\n",
    "                z = layers.fully_connected(z, num_outputs=512, activation_fn=leaky_relu)\n",
    "                z = layers.fully_connected(z, num_outputs=1024, activation_fn=leaky_relu)\n",
    "                z = layers.fully_connected(z, num_outputs=794, activation_fn=leaky_relu)\n",
    "                return z\n",
    "            \n",
    "        def discriminator(x, reuse, c_p=.01):\n",
    "            with tf.variable_scope('discriminator', reuse=reuse, constraint=lambda x: tf.clip_by_value(x,-c_p,c_p)):\n",
    "                x = layers.fully_connected(x, num_outputs=1024, activation_fn=leaky_relu)\n",
    "                x = layers.fully_connected(x, num_outputs=512, activation_fn=leaky_relu)\n",
    "                x = layers.fully_connected(x, num_outputs=256, activation_fn=leaky_relu)\n",
    "                x = layers.fully_connected(x, num_outputs=1, activation_fn=leaky_relu)\n",
    "                return x\n",
    "            \n",
    "    elif N==\"KCCFD\":\n",
    "        def generator(z):\n",
    "            with tf.variable_scope('generator'):\n",
    "                z = layers.fully_connected(z, num_outputs=512, activation_fn=leaky_relu)\n",
    "                z = layers.fully_connected(z, num_outputs=228, activation_fn=leaky_relu)\n",
    "                z = layers.fully_connected(z, num_outputs=30, activation_fn=tf.nn.sigmoid)\n",
    "                return z\n",
    "            \n",
    "        def discriminator(x, reuse, c_p=.01):\n",
    "            with tf.variable_scope('discriminator', reuse=reuse, constraint=lambda x: tf.clip_by_value(x,-c_p,c_p)):\n",
    "                x = layers.fully_connected(x, num_outputs=1024, activation_fn=leaky_relu)\n",
    "                x = layers.fully_connected(x, num_outputs=1024, activation_fn=leaky_relu)\n",
    "                x = layers.fully_connected(x, num_outputs=512, activation_fn=leaky_relu)\n",
    "                x = layers.fully_connected(x, num_outputs=1, activation_fn=leaky_relu)\n",
    "                return x\n",
    "            \n",
    "    elif N==\"KCCR\":\n",
    "        def generator(z):\n",
    "            with tf.variable_scope('generator'):\n",
    "                z = layers.fully_connected(z, num_outputs=128, activation_fn=leaky_relu)\n",
    "                z = layers.fully_connected(z, num_outputs=128, activation_fn=leaky_relu)\n",
    "                z = layers.fully_connected(z, num_outputs=36, activation_fn=tf.nn.sigmoid)\n",
    "                return z\n",
    "            \n",
    "        def discriminator(x, reuse, c_p=.01):\n",
    "            with tf.variable_scope('discriminator', reuse=reuse, constraint=lambda x: tf.clip_by_value(x,-c_p,c_p)):\n",
    "                x = layers.fully_connected(x, num_outputs=128, activation_fn=leaky_relu)\n",
    "                x = layers.fully_connected(x, num_outputs=128, activation_fn=leaky_relu)\n",
    "                x = layers.fully_connected(x, num_outputs=32, activation_fn=leaky_relu)\n",
    "                x = layers.fully_connected(x, num_outputs=1, activation_fn=leaky_relu)\n",
    "                return x\n",
    "            \n",
    "    return generator, discriminator\n",
    "\n",
    "\n",
    "class TF_model:\n",
    "    def __init__(self, name=None,T=\"WGAN\",dataset=\"MNIST\",batch_size=50,epsilon=1.0,overide_std=None):\n",
    "        assert T in [\"WGAN\",\"DPWGAN\"]\n",
    "        assert dataset in [\"MNIST\",\"KCCFD\",\"KCCR\"]\n",
    "        self.data=all_data[dataset]\n",
    "\n",
    "        self.name = (\"_\".join([x for x in [T,dataset,name] if x]).strip())\n",
    "        self.dataset = dataset\n",
    "        self.type = T\n",
    "        self.ckpt_name = \"models/\"+self.name+\"/model.ckpt\"\n",
    "        self.batch_size = batch_size\n",
    "        self.epsilon = epsilon\n",
    "        self.step = 0 # Track number of steps for saving.\n",
    "        self.n_d = 5 # Discriminator_iterations_per_step = 5\n",
    "        self.c_p = 0.01 # Clipping perameter\n",
    "        \n",
    "        # For DPWGAN, calcualted std for differential privacy.\n",
    "        if self.type == \"DPWGAN\": \n",
    "            if overide_std: self.std=overide_std\n",
    "            else: self.std = self.calculate_std(epsilon) \n",
    "            \n",
    "        self.graph = tf.Graph()\n",
    "        self.sess = tf.InteractiveSession(graph=self.graph)\n",
    "        self.make_model()\n",
    "        with self.graph.as_default():\n",
    "            self.saver = tf.train.Saver(max_to_keep=6)\n",
    "            \n",
    "            \n",
    "    # calculate std for noise to ensure differential privacy\n",
    "    def calculate_std(self,epsilon):\n",
    "        delta=10**(-8)\n",
    "        m=self.batch_size # Batch size\n",
    "        M=self.data.data_len   # Data  size\n",
    "        q=m/M # Sampling Probability\n",
    "\n",
    "        B_sigma = 1 # Upper bound on the activation function\n",
    "        B_sigma_prime = 1/4  # Upper bound on the derivitive activation function\n",
    "\n",
    "        # Number of connections\n",
    "        # Cheese this number by creating another model.\n",
    "        num_gradients = TF_model(dataset=self.dataset,T=\"WGAN\",batch_size=1).calc_num_grads()\n",
    "        print(\"self.c_p:\",self.c_p,\"B_sigma:\",B_sigma,\"B_sigma_prime:\",B_sigma_prime,\"num_gradients:\",num_gradients)\n",
    "        bound_of_dW_distance= 2*(self.c_p)*(B_sigma)*(B_sigma_prime**2)*num_gradients\n",
    "        print(\"bound_of_dW_distance\",bound_of_dW_distance)\n",
    "        print(\"q:\",q,\"self.n_d:\",self.n_d,\"delta:\",delta,\"epsilon:\",self.epsilon)\n",
    "        noise_scale=2*q*(self.n_d*math.sqrt(math.log(1/delta) / self.epsilon))\n",
    "        std = noise_scale*bound_of_dW_distance\n",
    "        print(\"std =\",std)\n",
    "        return std\n",
    "    \n",
    "    \n",
    "    def make_model(self):\n",
    "        global LR\n",
    "        with self.sess.as_default():\n",
    "\n",
    "            # Define noise layer\n",
    "            def get_DP_gradients(gradients):\n",
    "                noise = tf.random_normal(shape=tf.shape(gradients), mean=0.0,\n",
    "                                         stddev=self.std, dtype=tf.float32)\n",
    "                return gradients + noise\n",
    "\n",
    "            # Set placeholders\n",
    "            with tf.name_scope('placeholders'):\n",
    "                def get_placeholder(N):\n",
    "                    # input placeholder, this must be constucted withing the varible scope.\n",
    "                    \n",
    "                    if   N==\"MNIST\": return tf.placeholder(tf.float32, [None,  794])\n",
    "                    elif N==\"KCCFD\": return tf.placeholder(tf.float32, [None,  30])\n",
    "                    elif N==\"KCCR\":  return tf.placeholder(tf.float32, [None,  36])\n",
    "                self.z = tf.placeholder(tf.float32, [None, 128])\n",
    "                self.x_true = get_placeholder(self.dataset)\n",
    "\n",
    "            # Initialize nets\n",
    "            generator, discriminator = get_nets(self.dataset)\n",
    "            self.x_generated = generator(self.z)\n",
    "            self.d_true = discriminator(self.x_true, reuse=False, c_p=self.c_p)\n",
    "            self.d_generated = discriminator(self.x_generated, reuse=True, c_p=self.c_p)\n",
    "\n",
    "            with tf.name_scope('regularizer'):\n",
    "                self.epsilon = tf.random_uniform([self.batch_size, 1, 1, 1], 0.0, 1.0)\n",
    "                self.x_hat = self.epsilon * self.x_true + (1 - self.epsilon) * self.x_generated\n",
    "                self.d_hat = discriminator(self.x_hat, reuse=True)\n",
    "                self.gradients = tf.gradients(self.d_hat, self.x_hat, name=\"gradients\")[0]\n",
    "                if self.type == \"WGAN\":\n",
    "                    self.ddx = tf.sqrt(tf.reduce_sum(self.gradients ** 2, axis=[1, 2]))\n",
    "                elif self.type == \"DPWGAN\": # differential private gradients\n",
    "                    self.gradients = get_DP_gradients(self.gradients)\n",
    "                    self.ddx = tf.sqrt(tf.reduce_sum(self.gradients ** 2, axis=[1, 2]))\n",
    "                self.d_regularizer = tf.reduce_mean((self.ddx - 1.0) ** 2)\n",
    "\n",
    "            with tf.name_scope('loss'):\n",
    "                self.g_loss = tf.reduce_mean(self.d_generated)\n",
    "                self.d_loss = (tf.reduce_mean(self.d_true) - tf.reduce_mean(self.d_generated) +\n",
    "                          10 * self.d_regularizer)\n",
    "\n",
    "            with tf.name_scope('optimizer'):\n",
    "                self.optimizer = tf.train.AdamOptimizer(learning_rate=LR, beta1=0, beta2=0.9)\n",
    "                self.g_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='generator')\n",
    "                self.g_train = self.optimizer.minimize(self.g_loss, var_list=self.g_vars)\n",
    "                self.d_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='discriminator')\n",
    "                self.d_train = self.optimizer.minimize(self.d_loss, var_list=self.d_vars)\n",
    "                \n",
    "            tf.global_variables_initializer().run()\n",
    "            \n",
    "            \n",
    "    def train(self,n=20000,save_every=20):\n",
    "        with self.graph.as_default():\n",
    "            for i in range(1,n+1):\n",
    "                # Inputs\n",
    "                batch = self.data.next_batch(self.batch_size)\n",
    "                z_train = np.random.randn(self.batch_size, 128)\n",
    "                # g-train\n",
    "                g_loss, _ =self.sess.run([self.g_loss,self.g_train], feed_dict={self.z: z_train})\n",
    "                # d-train\n",
    "                for j in range(self.n_d): d_loss,_=self.sess.run([self.d_loss,self.d_train],  feed_dict={self.x_true: batch, self.z: z_train})\n",
    "                # Bookkeeping\n",
    "                self.step+=1\n",
    "                if i % 20 == 0 or i==n: print(\"d_loss:\",d_loss,\"| g_loss:\",g_loss,end=\" |\")\n",
    "                if (save_every and i % 20 == 0) or i==n: self.save_samples()\n",
    "                if i % 100 == 0 or i==n:\n",
    "                    print('train iter={}/{}, step={}'.format(i,n,self.step))\n",
    "                    self.generate(1)\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def calc_num_grads(self):\n",
    "        with self.graph.as_default():\n",
    "            from functools import reduce\n",
    "            def mult(shape): return reduce(lambda x, y: x*y, shape)\n",
    "            # Inputs\n",
    "            batch = self.data.next_batch(self.batch_size)\n",
    "            z_train = np.random.randn(self.batch_size, 128)\n",
    "            # Get a set of gradients    \n",
    "            grads=self.sess.run(self.gradients, feed_dict={self.x_true: batch, self.z: z_train})\n",
    "            # Count them up\n",
    "            total_gradients = 0\n",
    "            for gradient in grads:\n",
    "                print(gradient.shape)\n",
    "                variable_gradients=mult(gradient.shape)\n",
    "                print(\"max grad:\",np.amax(gradient),\"| min grad:\",np.amin(gradient))\n",
    "                print(variable_gradients)\n",
    "                total_gradients += variable_gradients\n",
    "            print(\"total_gradients =\",total_gradients/self.batch_size)\n",
    "            return total_gradients\n",
    "        \n",
    "        \n",
    "    def generate(self,n=5):\n",
    "        with self.graph.as_default():\n",
    "            if self.dataset == \"MNIST\":\n",
    "                z_validate = np.random.randn(n, 128)\n",
    "                generated = self.x_generated.eval(feed_dict = {self.z: z_validate}).squeeze()\n",
    "                imgs,lables =  np.reshape(generated[:28*28],(-1,28,28)),generated[28*28:]\n",
    "                self._plot(imgs,lables)\n",
    "                if len(lables.shape) == 1: self._plot(imgs,lables)\n",
    "                else:\n",
    "                    for i in len(lables): self._plot(imgs[i],lables[i])\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def save_samples(self,batch_size=50,batches=120):\n",
    "        with self.graph.as_default():\n",
    "            samples=[]\n",
    "            for i in range(batches):\n",
    "                z_validate = np.random.randn(batch_size, 128)\n",
    "                generated = self.x_generated.eval(feed_dict = {self.z: z_validate}).squeeze()\n",
    "                samples+=[x.tolist() for x in generated]\n",
    "        samples=np.asarray(samples)\n",
    "        num_classes = 1\n",
    "        if self.dataset == \"MNIST\": num_classes = 10\n",
    "        save_file=\"DATA/GENERATED/\"+self.name+\"_GENERATED.csv\"\n",
    "        save_data(samples, save_file, attribute_names[self.dataset], num_classes)\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def save_200_samples(self,batch_size=50,batches=40):\n",
    "        with self.graph.as_default():\n",
    "            assert batch_size * batches==2000\n",
    "            samples=[]\n",
    "            for i in range(batches):\n",
    "                z_validate = np.random.randn(batch_size, 128)\n",
    "                generated = self.x_generated.eval(feed_dict = {self.z: z_validate}).squeeze()\n",
    "                samples+=[x.tolist() for x in generated]\n",
    "        samples=np.asarray(samples)\n",
    "        num_classes = 1\n",
    "        if self.dataset == \"MNIST\": num_classes = 10\n",
    "        save_file=\"DATA/GENERATED/\"+self.name+\"_GENERATED.csv\"\n",
    "        save_data(samples, save_file, attribute_names[self.dataset], num_classes)\n",
    "        print(\"Samples saved:\",save_file)\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def _plot(self, imgs,lables):\n",
    "        if len(lables.shape) == 1: i_max=1;imgs=[imgs],lables=[lables]\n",
    "        else: i_max=lables.shape[0]\n",
    "        fig, ax = plt.subplots(1, i_max, figsize=(12, 6))\n",
    "        for i in range(i_max):\n",
    "            lable=lables[i]\n",
    "            print(\"Class =\",np.argmax(lable),\"class array:\",[round(x,2) for x in lable])\n",
    "            ax[i].set_title(\"class=\"+str(np.argmax(lable)))\n",
    "            ax[i].imshow(np.reshape(img,(28,28)), clim=[0, 1], cmap='bone')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "    def save(self):\n",
    "        with self.graph.as_default():\n",
    "            self.saver.save(self.sess, self.ckpt_name, global_step=self.step)\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def load(self,path=None):\n",
    "        #this probably doesn't work\n",
    "        with self.graph.as_default():\n",
    "            if path:\n",
    "                print(tf.train.latest_checkpoint(self.name+\"/checkpoint\"))\n",
    "                print(os.path.join(self.name, path))\n",
    "                self.saver.restore(self.sess, os.path.join(self.name, path))\n",
    "                print(\"Loaded sucessfully\")\n",
    "            else:\n",
    "                print(self.name)\n",
    "                ckpt = tf.train.get_checkpoint_state(self.name)\n",
    "                print(os.path.basename(ckpt.model_checkpoint_path))\n",
    "                if ckpt and ckpt.model_checkpoint_path:\n",
    "                    print(ckpt.model_checkpoint_path)\n",
    "                    self.saver.restore(self.sess, os.path.join(path, os.path.basename(ckpt.model_checkpoint_path)))\n",
    "                    print(\"Loaded sucessfully\")\n",
    "                else:\n",
    "                    print(\"Load error: No checkpoint at \"+self.name)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_WGAN(name=None,batch_size=20,iterations=2000):\n",
    "    global LR\n",
    "    LR=2*1e-4\n",
    "    model_TEST1=TF_model(T=\"WGAN\", name=name ,dataset=\"KCCR\", batch_size=batch_size).train(iterations).save().save_samples()\n",
    "    LR=5*1e-5\n",
    "    model_TEST2=TF_model(T=\"WGAN\", name=name ,dataset=\"KCCFD\",batch_size=batch_size).train(iterations).save().save_samples()\n",
    "    model_TEST3=TF_model(T=\"WGAN\", name=name ,dataset=\"MNIST\",batch_size=batch_size).train(iterations,None).save().save_samples()\n",
    "# train_WGAN(name=\"my_data\")\n",
    "\n",
    "def train_DPWGAN(name=None,batch_size=20,epsilon=1,iterations=2000):\n",
    "    global LR\n",
    "    LR=2*1e-4\n",
    "    model_TEST4=TF_model(T=\"DPWGAN\",dataset=\"KCCR\", name=name,batch_size=batch_size,\n",
    "                 epsilon=epsilon).train(iterations,100).save().save_samples()\n",
    "    LR=5*1e-5\n",
    "    model_TEST5=TF_model(T=\"DPWGAN\",dataset=\"KCCFD\",name=name,batch_size=batch_size,\n",
    "                 epsilon=epsilon).train(iterations,100).save().save_samples()\n",
    "    model_TEST6=TF_model(T=\"DPWGAN\",dataset=\"MNIST\",name=name,batch_size=batch_size,\n",
    "                 epsilon=epsilon).train(iterations,100).save().save_samples()\n",
    "# train_DPWGAN(name=\"my_data\")\n",
    "\n",
    "def train_all_DPWGANs(batch_size=20,iterations=2000):\n",
    "    global LR\n",
    "    for epsilon in [0.01, 0.05, 0.1, 0.5, 1.0, 5.0, 10.0][::-1]:\n",
    "        train_DPWGAN(name=\"eps=\"+str(epsilon),batch_size=batch_size,epsilon=epsilon,iterations=iterations)\n",
    "# train_all_DPWGANs(name=\"my_data\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
